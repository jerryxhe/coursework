{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 Binomial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y_{1},...,Y_{n}$ are indepdendent random variables $Y_{i}\\sim Bin(n_{i},p_{i})$\n",
    "$$\n",
    "g(p_{i})=\\beta^{T}x_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $g$ is the canonical link $\\log\\left(\\frac{p_{i}}{1-p_{i}}\\right)=\\beta^{T}x_{i}$\n",
    ". ($\\mbox{logit}(p_{i})$ or log odds), which means that \n",
    "\n",
    "$$p_{i}=p_{i}(\\beta)=\\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x_{i}}}$$. \n",
    "\n",
    "and\n",
    "\n",
    "$$likelihood=\\prod_{i=1}^{n}\\left(\\begin{array}{c}\n",
    "n_{i}\\\\\n",
    "y_{i}\n",
    "\\end{array}\\right)p_{i}^{y_{i}}(1-p_{i})^{n-y_{i}}$$\n",
    "\n",
    "So $log (likelihood) $ =\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "l(\\beta) & = & \\sum_{i=1}^{n}\\left(y_{i}\\log\\,p_{i}+(n_{i}-y_{i})\\log(1-p_{i})+\\log\\left(\\begin{array}{c}\n",
    "n_{i}\\\\\n",
    "y_{i}\n",
    "\\end{array}\\right)\\right)\\\\\n",
    " & = & \\sum_{i=1}^{n}\\left(y_{i}\\log\\left(\\frac{p_{i}}{1-p_{i}}\\right)+n_{i}\\log(1-p_{i})+\\log\\left(\\begin{array}{c}\n",
    "n_{i}\\\\\n",
    "y_{i}\n",
    "\\end{array}\\right)\\right)\\\\\n",
    " & = & \\beta^{T}\\left(\\sum_{i=1}^{n}y_{i}x_{i}\\right)-n_{i}\\log\\left(1+e^{\\beta^{T}x_{i}}\\right)+\\sum_{i=1}^{n}\\log\\left(\\begin{array}{c}\n",
    "n_{i}\\\\\n",
    "y_{i}\n",
    "\\end{array}\\right)\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial l}{\\partial\\beta}=\\sum_{i=1}^{n}y_{i}x_{i}-\\sum_{i=1}^{n}n_{i}x_{i}\\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}}\n",
    "$$\n",
    "So mle $\\hat{\\beta}$ solves $\\sum y_{i}x_{i}=\\sum n_{i}x_{i}\\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}}$\n",
    "(Solve via iteration see H51)\n",
    "$$\n",
    "\\frac{\\partial^{2}l}{\\partial\\beta\\partial\\beta^{T}}=-\\sum_{i}n_{i}x_{i}x_{i}^{T}\\frac{e^{\\beta^{T}x_{i}}}{\\left(1+e^{\\beta^{T}x_{i}}\\right)^{2}}\n",
    "$$\n",
    " (check) Does not depend on $Y_{i}$'s So\n",
    "$$\n",
    "\\mathbb{E}\\left[-\\frac{\\partial^{2}l}{\\partial\\beta\\partial\\beta^{T}}\\right]=\\sum n_{i}x_{i}x_{i}^{T}p_{i}(1-p_{i})=V(\\beta)^{-1}\n",
    "$$\n",
    " say. Then, by general theory for MLE's, we have\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}\\sim N\\left(\\beta,\\,V(\\beta)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall $\\varphi_{i}=\\frac{1}{n_{i}}$ ie of form $a_{i}\\varphi$ with\n",
    "$a_{i}'s$ known, $a_{i}=\\frac{1}{n_{i}}$, $\\varphi=1$ \n",
    "\n",
    "### Deviance\n",
    "\n",
    "saturated model $w_{s}$, $Y_{i}\\sim Bin(n_{i},\\,p_{i})$ independent\n",
    "$0<p_{i}<1$\n",
    "\n",
    "For $\\omega_{s}$, $\\frac{\\partial l}{\\partial p_{i}}=0$ in (8.1)\n",
    "gives us $\\tilde{p}_{i}=\\frac{y_{i}}{n_{i}}$. So \n",
    "$$\n",
    "l_{\\max}^{(s)}=\\sum\\left\\{ y_{i}\\log\\left(\\frac{y_{i}}{n_{i}}\\right)+\\left(n_{i}-y_{i}\\right)\\log\\left(\\frac{n_{i}-y_{i}}{n_{i}}\\right)+\\log\\left(\\begin{array}{c}\n",
    "n_{i}\\\\\n",
    "y_{i}\n",
    "\\end{array}\\right)\\right\\} \n",
    "$$\n",
    "\n",
    "\n",
    "$\\omega_{f}$ $\\log\\left(\\frac{p_{i}}{1-p_{i}}\\right)=\\beta^{T}x_{i}$\n",
    "let mle's be $\\hat{p}_{i}=p_{i}\\left(\\hat{\\beta}\\right)$. From (8.1)\n",
    "\n",
    "$$\n",
    "l_{\\max}^{(s)}=\\sum\\left\\{ y_{i}\\log\\left(p_{i}\\left(\\hat{\\beta}\\right)\\right)+\\left(n_{i}-y_{i}\\right)\\log\\left(1-p_{i}\\left(\\hat{\\beta}\\right)\\right)+\\log\\left(\\begin{array}{c}\n",
    "n_{i}\\\\\n",
    "y_{i}\n",
    "\\end{array}\\right)\\right\\} \n",
    "$$\n",
    "\n",
    "\n",
    "so scaled deviance\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "S\\left(\\omega_{f},\\,\\omega_{s}\\right) & = & 2\\left\\{ l_{\\max}^{(s)}-l_{\\max}^{(f)}\\right\\} \\\\\n",
    " & = & \\sum\\left\\{ y_{i}\\log\\left(\\frac{y_{i}}{e_{i}}\\right)+\\left(n_{i}-y_{i}\\right)\\log\\left(\\frac{n_{i}-y_{i}}{n_{i}-e_{i}}\\right)\\right\\} \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "where $e_{i}=n_{i}p_{i}\\left(\\hat{\\beta}\\right)=n_{i}\\hat{p}_{i}$\n",
    "are the estimated expectd values under the model $\\omega_{f}$. Here\n",
    "$\\varphi=1$, so deviance is $D\\left(\\omega_{f},\\,\\omega_{s}\\right)=\\varphi S\\left(\\omega_{f},\\,\\omega_{s}\\right)=S\\left(\\omega_{f},\\omega_{s}\\right)$.\n",
    "It can be shown (expansion for log) that (8.2) is approximately\n",
    "\n",
    "$$\n",
    "\\sum_{i}\\frac{\\left(y_{i}-e_{i}\\right)^{2}}{y_{i}}+\\frac{\\left(\\left(n_{i}-y_{i}\\right)-\\left(n_{i}-e_{i}\\right)\\right)^{2}}{n_{i}-e_{i}}\n",
    "$$\n",
    "\n",
    "this is Pearson's $\\chi^{2}$ for binomial data since $\\sum\\frac{\\left(O-E\\right)^{2}}{E}$\n",
    "are approximately chi squared distribution. \n",
    "\n",
    "Both this and deviance are approximately $\\chi_{n-p}^{2}$ if $\\omega_{f}$\n",
    "is true. So if $\\omega_{f}$ is a bad fit then deviance will be large\n",
    "compared to $\\chi_{n-p}^{2}$. \n",
    "\n",
    "Logistic link is the most commonly used, other possibilities \n",
    "$$\n",
    "g(p_{i})=\\Phi^{-1}(p_{i})\\mbox{ probit link}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g\\left(p_{i}\\right)=\\log\\left(-\\log\\left(1-p_{i}\\right)\\right)\\,\\,\\,\\,\\,\\mbox{ complementary log log}\n",
    "$$\n",
    "\n",
    "Example logistic regression see H6.1\n",
    "\n",
    "Nov 20th, 2:30pm Shell statistician talk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson data\n",
    "\n",
    "$Y\\sim Poisson(\\mu)$, $\\mathbb{P}(Y=y)=\\frac{e^{-\\mu}\\mu^{y}}{y!}=\\exp\\left\\{ y\\log\\left(\\mu\\right)+\\mu-\\log\\left(y!\\right)\\right\\} $\n",
    "\n",
    "$\\Theta=\\log\\mu$, $b(\\theta)=\\mu=e^{\\theta}$, $\\varphi=1$ (check)\n",
    "\n",
    "Check that $b'(\\theta)=\\mu$ and $V(\\mu)=\\mu$\n",
    "\n",
    "Canonical Link: $g(\\mu)=\\log\\left(\\mu\\right)$\n",
    "\n",
    "$Y_{1},..,Y_{n}$ independent $Y_{i}\\sim Poisson\\left(\\mu_{i}\\right)$\n",
    "\n",
    "Assume canonical link $\\log\\left(\\mu_{i}\\right)=\\beta^{T}x_{i}$ so\n",
    "$\\mu_{i}=e^{\\beta^{T}x_{i}}=\\mu_{i}(\\beta)$ say\n",
    "\n",
    "Loglikelihood is\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "l(\\beta) & = & \\sum_{i=1}^{n}\\left\\{ -\\mu_{i}+y_{i}\\log\\mu_{i}-\\log\\left(y_{i}!\\right)\\right\\} \n",
    "& = & -\\sum_{i=1}^{n}e^{\\beta^{T}x_{i}}+\\beta^{T}\\sum_{i=1}^{n}y_{i}x_{i}-\\sum_{i=1}^{n}\\log\\left(y_{i}!\\right)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial\\beta}=-\\sum_{i=1}^{n}x_{i}e^{\\beta^{T}x_{i}}+\\sum_{i=1}^{n}y_{i}x_{i}\n",
    "$$\n",
    "\n",
    "\n",
    "mle $\\hat{\\beta}$ satisfies eq:9.2\n",
    "$$\n",
    "\\sum_{i=1}^{n}y_{i}x_{i}=\\sum_{i=1}^{n}e^{\\hat{\\beta}^{T}x_{i}}x_{i}\n",
    "$$\n",
    "\n",
    "\n",
    "Also $-\\frac{\\partial^{2}l}{\\partial\\beta\\partial\\beta^{T}}=\\sum_{i=1}^{n}x_{i}x_{i}^{T}e^{\\beta^{T}x_{i}}=V(\\beta)^{-1}$\n",
    "say so mle $\\hat{\\beta}\\sim N(\\beta,V(\\beta))$, we may replace $V(\\beta)$\n",
    "by $V(\\hat{\\beta})$.\n",
    "\n",
    "Saturated model $\\omega_{s}:$ $Y_{i}\\sim Poisson(\\mu_{i})$ $\\mu_{i}>0$\n",
    "\n",
    "Check that $\\hat{\\mu}_{i}^{(s)}=y_{i}$. Fitted model ($\\log\\left(\\mu_{i}\\right)=\\beta^{T}x_{i}$)\n",
    "$\\omega_{f}$, $\\hat{\\mu}_{i}^{(f)}=\\mu_{i}\\left(\\hat{\\beta}\\right)$\n",
    "So\n",
    "\n",
    "$$\n",
    "l_{\\max}^{(s)}=\\sum_{i=1}^{n}\\left\\{ y_{i}\\log\\left(y_{i}\\right)-y_{i}-\\log\\left(y_{i}!\\right)\\right\\} \n",
    "$$\n",
    "\n",
    "\n",
    "from (9.1) and \n",
    "$$\n",
    "l_{\\max}^{(f)}=\\sum_{i=1}^{n}\\left\\{ y_{i}\\log\\left(\\mu_{i}\\left(\\hat{\\beta}\\right)\\right)-\\mu_{i}\\left(\\hat{\\beta}\\right)-\\log\\left(y_{i}!\\right)\\right\\} \n",
    "$$\n",
    "\n",
    "\n",
    "Recall $\\varphi=1$ , so \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "D & = & \\mbox{deviance}=\\mbox{scaled deviance}\\\\\n",
    "& = & 2\\left(l_{\\max}^{(s)}-l_{\\max}^{(f)}\\right)\\\\\n",
    "& = & 2\\left\\{ \\sum_{i=1}^{n}y_{i}\\log\\left(\\frac{y_{i}}{\\mu_{i}\\left(\\hat{\\beta}\\right)}\\right)-\\sum_{i=1}^{n}\\left(y_{i}-\\mu_{i}\\left(\\hat{\\beta}\\right)\\right)^{2}\\right\\} \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Assess the fit of $\\omega_{f}$ by comparing $D$ to $\\chi_{n-p}^{2}$.\n",
    "\n",
    "Note: Using $s\\log\\left(\\frac{s}{t}\\right)\\approx(s-t)+\\frac{(s-t)^{2}}{2t}$\n",
    "for small $\\left|s-t\\right|$ (check)\n",
    "\n",
    "$$\n",
    "D\\approx2\\left\\{ \\sum_{i=1}^{n}\\left[\\left(y_{i}-\\mu_{i}\\left(\\hat{\\beta}\\right)\\right)+\\frac{\\left(y_{i}-\\mu_{i}\\left(\\hat{\\beta}\\right)\\right)^{2}}{2\\mu_{i}\\left(\\hat{\\beta}\\right)}\\right]-\\sum_{i=1}^{n}\\left(y_{i}-\\mu_{i}\\left(\\hat{\\beta}\\right)\\right)\\right\\} \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "D\\approx\\sum_{i=1}^{n}\\frac{\\left(y_{i}-\\mu_{i}\\left(\\hat{\\beta}\\right)\\right)^{2}}{\\mu_{i}\\left(\\hat{\\beta}\\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "Pearson's $\\chi^{2}$ $o_{i}=y_{i}$ , $e_{i}=\\mu_{i}(\\hat{\\beta})$\n",
    "\n",
    "#### Residuals\n",
    "\n",
    "Pearson residual is $r_{i}=\\frac{y_{i}-e_{i}}{\\sqrt{e_{i}}}$ $e_{i}=\\mu_{i}\\left(\\hat{\\beta}\\right)$,\n",
    "$\\chi^{2}=\\sum_{i=1}^{n}r_{i}^{2}$\n",
    "\n",
    "Deviance residuals: $d_{i}=\\mbox{sign}(y_{i}-e_{i})\\sqrt{2\\left[y_{i}\\log\\left(\\frac{y_{i}}{e_{i}}\\right)-(y_{i}-e_{i})\\right]}$,\n",
    "so $D=\\sum_{i=1}^{n}d_{i}^{2}$\n",
    "\n",
    "See H7.1\n",
    "\n",
    "### Using Poisson for rates\n",
    "\n",
    "Suppose $Y_{1}$, ..., $Y_{n}$ are counts for different ``exposures'',\n",
    "$m_{1},...,m_{n}$. \n",
    "\n",
    "$Y_{i}\\sim$Poisson$\\left(\\mu_{i}\\right)$ where $\\mu_{i}=m_{i}\\theta_{i}$\n",
    "$\\theta_{i}$ is \\uline{rate} .\n",
    "\n",
    "Interest lies in modelling the rate. i.e. in modelling how $\\theta_{i}$\n",
    "depends e.g. on covariates. \n",
    "\n",
    "Model $\\log(\\mu_{i})=\\log(m_{i}\\theta_{i})=\\log(m_{i})+\\log(\\theta_{i})$ \n",
    "\n",
    "In this type of situation $\\log\\left(m_{i}\\right)$ is an \\uline{offset\n",
    "}. Its coefficient is forced to be 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
