{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general form of a mixture model is \n",
    "$$ p(x|\\theta) = \\sum_{m=1}^M p(x, w_m \\vert \\theta_m) = \\sum_{m=1}^M c_m p(x \\vert w_m \\theta_m) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Variables\n",
    "The set of variables $\\mathbb{Z}$ are called *hidden* or *latent* variables. They may be discrete variable or continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of data $ \\{Z, x\\}$ is sometimes referred to as the complete dataset. It consists of the observed data x (the feature vectors) and unobserved data Z( the hidden variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Models & Expectation Maximization\n",
    "Mixture models of a particular family of distributions are very well suited for estimation using EM (e.g. Gaussian Poisson etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a discrete hidden variable to indicate which of the components of the mixture model generated by an observation:\n",
    "$$ z_{ij} = \\cases{1,&observation $x_i$ was generated by component $w_j$ \\cr 0,&otherwise \\cr}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's Discriminant Analysis\n",
    "A different approach to training the parameters of the perceptron is to use Fisher's discriminant analysis. The basic aim here is to choose the *projection* that maximises the distance between the class means, whilst minimising the within class variance. Note only the projection $w$ is determine. The following cost function is used\n",
    "$$ E(w) = - \\frac{(\\bar\\mu_1 - \\bar\\mu_2)^2}{\\bar s_1 + \\bar s_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matern covariance functions\n",
    "Stationary covariance functions can be based on the Matern form\n",
    "$$ k(x,x') = \\frac{1}{\\Gamma(\\nu) 2^{\\nu-1}} \\left[ \\frac{\\sqrt{2\\nu}}{\\ell} \\vert x-x' \\vert \\right]^\\nu K_\\nu \\left( \\frac{\\sqrt{2\\nu}}{\\ell} \\vert x-x' \\vert  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special cases\n",
    "* $k_{\\nu=1/2}(r) = \\exp (-\\frac{r}{\\ell}) $ Laplacian covariance function, Brownian motion (Ornstein-Uhlenbeck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity Measures\n",
    "* **Entropy** measure $$ \\mathcal{I}(N) = - \\sum_{i=1}^K P(w_i \\big\\vert N) \\log \\left( P(w_i \\vert N) \\right) $$ \n",
    "* **Gini** measure: this is a generalization of the variance measure to K classes $$ \\mathcal{I}(N) = 1 - \\sum_{i=1}^K P(w_i \\big\\vert N)^2 $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
