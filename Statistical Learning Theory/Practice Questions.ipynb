{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What is the Bayes' minimum error rate classification rule for a multi-class problem? What determines how closely a practical classifier approaches Bayes' mininum error rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "For multi-class problems, we calculate all the C posterior probabilities\n",
    "$$ P(w_1|x), P(w_2|x),...,P(w_C|x) $$\n",
    "find the maximum of these and assign the vector $x$ to the corresponding class\n",
    "Applying Bayes' Rule t this formula we need to find the class $w_j$ which gives\n",
    "$$ \\max_j p(x|w_j)P(w_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Class-conditional probability density functions are to be estimated for a supervised training task. A 20-dimensional feature vector is used. The class-conditional PDFs are thought to be approximately Gaussian distributed and have class-dependent correlations between ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What are the advantages of using expectation maximisation (EM) rather than gradient descent based methods for estimating the parameters of a GMM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "When E-M is used on each iteration the maximisation will guarantee an increase in the log likelihood of the data unless it is at a local maximum. Hence a graph of the log likelihood against iteration will monotonically increase and converge. Furthermore, no information about pervious iterations updates is necessary (unlike the use of a momentum term)\n",
    "For gradient-descent (this would be on the negative of the log likelihood function), the step size for all parameters has to be explicitly set and there is no guarantee that the likelihood will increase. For stability, we need to use a small step size. The training procedure can be improved by the use of information from previous iterations or second order methods but these require more statistics to be computed than for E-M. Hence E-M is preferred for problems to which it can be applied. \n",
    "Not that both are susceptible to local maxima in the log likelihood function.\n",
    "(d) (i) The log likelihood function for the data\n",
    "$$ \\ell (\\theta) = \\sum_{k=1}^n \\ln p\\left(x_k\\right) =  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Using the method of Lagrange multipliers, find an expression that must be satisfied for the maximum likelihood of the component priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "In this case add $ \\lambda \\left(\\sum_{m=1}^M c_m - 1\\right) $ to the log likelihood function and repeat as above and equate to zero for a maximum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "A multi-layer perceptron (MLP) is to be used for a classification task. There are $C$ possible classes. The MLP has d-dimensional input, an output layer, $L$ hidden layers, and $N^{(k)}$ units in the $k^{th}$ hidden layer.\n",
    "\n",
    "What is the purpose of each layer of the MLP? Discuss the issues that must be considered when selecting the number of network layers and ndoes per layer for a particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "The number of hidden layers determines the decision boundaries that can be generated. In choosing the number of hidden layers it should be noted that multilayer networks are harder to train than single layer networks and that a two layer network (one hidden) with sigmoidal activation functions can model any decision boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore if a problem is linearly separable (rare for real problems), choose a single layer L=0 network. Two layer networks L=1 are most commonly used in pattern recognition (the hidden layer having sigmoidal activation functions). The number of nodes in the hidden layer needs to be great enough to be able to construct the decision boundaries for that problem but limited otherwise the number of weight will be too large to estimate reliably and performance on test-data (i.e. generalisation) will suffer. \n",
    "For classification tasks the input has the number of inputs equal to the dimensionality of the data, and the output layer normally has as many nodes as classes (1-in-C coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "$$ y_1,...,y_n \\sim  \\mathcal{N} \\left( \\theta , 1 \\right) $$\n",
    "Let $P(\\theta) \\propto 1$\n",
    "What is posterior distribution for $\\theta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Suppose we have\n",
    "$$ y_1,y_2, ...,y_n \\sim Binomial\\left(1, \\theta \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "The input to the activation function is continuous. FOr the following activation functions give the equation of the function and the form of the output:\n",
    " (i) heaviside function\n",
    " (ii) sigmoid function\n",
    " (iii) softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "(i) $$y_i = \\cases{1,& $z_i \\geq 0$ \\cr 0,&  $z_i <0 $\\cr} $$\n",
    "(ii) $$ y_j = \\frac{1}{1+\\exp\\left(-z_j\\right)} $$\n",
    "(iii) Softmax function $$ y_j = \\frac{\\exp\\left(z_j\\right)}{\\sum_{i=1}^n \\exp{z_i}} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
