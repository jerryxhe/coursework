{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What is the Bayes' minimum error rate classification rule for a multi-class problem? What determines how closely a practical classifier approaches Bayes' mininum error rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "For multi-class problems, we calculate all the C posterior probabilities\n",
    "$$ P(w_1|x), P(w_2|x),...,P(w_C|x) $$\n",
    "find the maximum of these and assign the vector $x$ to the corresponding class\n",
    "Applying Bayes' Rule t this formula we need to find the class $w_j$ which gives\n",
    "$$ \\max_j p(x|w_j)P(w_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Class-conditional probability density functions are to be estimated for a supervised training task. A 20-dimensional feature vector is used. The class-conditional PDFs are thought to be approximately Gaussian distributed and have class-dependent correlations between ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What are the advantages of using expectation maximisation (EM) rather than gradient descent based methods for estimating the parameters of a GMM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "When E-M is used on each iteration the maximisation will guarantee an increase in the log likelihood of the data unless it is at a local maximum. Hence a graph of the log likelihood against iteration will monotonically increase and converge. Furthermore, no information about pervious iterations updates is necessary (unlike the use of a momentum term)\n",
    "For gradient-descent (this would be on the negative of the log likelihood function), the step size for all parameters has to be explicitly set and there is no guarantee that the likelihood will increase. For stability, we need to use a small step size. The training procedure can be improved by the use of information from previous iterations or second order methods but these require more statistics to be computed than for E-M. Hence E-M is preferred for problems to which it can be applied. \n",
    "Not that both are susceptible to local maxima in the log likelihood function.\n",
    "(d) (i) The log likelihood function for the data\n",
    "$$ \\ell (\\theta) = \\sum_{k=1}^n \\ln p\\left(x_k\\right) =  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Using the method of Lagrange multipliers, find an expression that must be satisfied for the maximum likelihood of the component priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "In this case add $ \\lambda \\left(\\sum_{m=1}^M c_m - 1\\right) $ to the log likelihood function and repeat as above and equate to zero for a maximum. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
