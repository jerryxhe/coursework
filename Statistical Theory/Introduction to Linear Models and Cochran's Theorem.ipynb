{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give an example of marginally normal components, but not a multivariate normal. \n",
    "Suppose that $Y=X$ if $|X|\\le a$ and $Y=-X$ otherwise. If $X$ is a standard normal, then so is $Y$, but they are not a MVN as $X+Y$ is not normal. \n",
    "\n",
    "Exercise: If $Y\\sim N_n(\\mu,\\Sigma)$ and $A$ is an arbitrary $p\\times n$ matrix, then $AY\\sim N_p(A\\mu, A\\Sigma A^T)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Model\n",
    "\n",
    "Suppose $Y_1,\\ldots, Y_n$ are independent responses of the form $Y_i\\sim N(\\mu_i,\\sigma^2)$, where $\\mu_i=X_i^T\\beta$ for a known vector of explanatoryvariables $X_i^T=(X_{i1},\\ldots, X_{ip})$ andunknown vector of regression coefficients $\\beta =(\\beta_1,\\ldots,\\beta_p)^T$. \n",
    "\n",
    "The linear model is usually written $Y=X\\beta+\\epsilon$ where $Y=(y_1,\\ldots, Y_n)^T$ and $X=(X_1^T,\\ldots, X_n^T)^T$ and $\\epsilon=(\\epsilon_1,\\ldots, \\epsilon_n)^T$ where $\\epsilon\\sim N_n(0,\\sigma^2I)$. It is usual to assume that the $n\\times p$ **design matrix** $X$ has full rank,  so the $p\\times p$ matrix $X^TX$ is positive definite if it is invertible. This is because $z\\in\\mathbb{R}^p$ is nonzero, then $z^TX^TXz = \\|Xz\\|^2 >0$\n",
    "\n",
    "In this model, the unknown parameter is $\\theta=(\\beta, \\sigma^2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "\n",
    "* *(i)* Show that $\\hat\\beta = (X^TX)^{-1} X^TY$ and $\\hat\\beta\\sim N_p(\\beta, \\sigma^2(X^TX)^{-1})$\n",
    "\n",
    "* *(ii)* Show that $\\hat\\sigma^2 = \\frac{1}{n}\\|Y-X\\hat\\beta\\|^2$ and $\\hat \\sigma^2 \\sim \\frac{\\sigma^2}{n}\\chi_{n-p}^2$.\n",
    "\n",
    "* *(iii)* Show that $\\hat \\beta, \\hat \\sigma^2$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cochran's Theorem\n",
    "Let $Y\\sim N_n(0,\\sigma^2I)$ and suppose that $A_1,\\ldots, A_k$ are $n\\times n$ symmetric matrices with $\\text{rank}(A_i)=r_i$. If $r_1+\\ldots =r_k=n$, then $Y^TA_iY\\sim \\sigma^2 \\chi_r^2$ and $Y^TA_1Y,\\ldots, Y^TA_kY$ are independent. \n",
    "\n",
    "\n",
    "Note that the fitted values $\\hat Y= X\\hat \\beta$ satisfy $\\hat Y = X(X^TX)^{-1}X^TY=PY$, where $P=X(X^TX)^{-1}X^T$ is a matrix representing an orthogonal projection, that is $P^2=P$. \n",
    "\n",
    "It maps $Y$ onto the $p$-dimensional subspace consisting of $U=\\{Xb:b\\in\\mathbb{R}^p\\}$, so it has rank $p$. (WHY?)\n",
    "\n",
    "Now partition $X=(X_0, X_1)$ and $\\beta =(\\beta_0, \\beta_1)^T$ where $\\beta_0$ is an $n\\times p_0$ matrix. We will do a hypothesis test. $H_0:\\beta_1=0$ versus $H_1:\\beta_1\\not = 0$. We will compare the likelihood ratios. For $H_1$, we already have the MLE and for $H_0$, the MLE of $\\beta_0 = \\hat{\\hat{\\beta_0}}$. Note: $\\hat{\\beta_0} = \\hat{\\hat{ \\beta_0}}$ iff elements in the covariance matrix of $\\hat{\\beta}$ corresponding to $\\hat{\\beta_1}$ are orthogonal. \n",
    "\n",
    "Note $\\hat{\\hat{Y}} = X_0 \\hat{\\hat{\\beta_0}} = P_0Y$. Moreover, $\\hat{\\hat{\\sigma}}^2 = \\frac{1}{n}\\|Y-X_0\\hat{\\hat{\\beta_0}}\\|^2$ and $\\hat{\\hat{\\sigma}}^2$, $\\hat{\\hat{\\beta}}$ are independent. The likelihood ratio statistics is \n",
    "$$\\ell(\\beta,\\sigma^2) = -\\frac{n}{2}\\log \\sigma^2-\\frac{1}{2\\sigma^2}\\|Y-X\\beta\\|^2$$\n",
    "$$w_{LR}(H_0) = 2(\\ell(\\hat{\\beta}, \\hat{\\sigma})-\\ell(\\hat{\\hat{\\beta_0}}, \\hat{\\hat{\\sigma_0}})) = n\\log(\\frac{\\|Y-P_0Y\\|^2}{\\|Y-PY\\|^2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 3\n",
    "We have two linear models. One is larger, one is smaller, which do we prefer? We look at the likelihood ratio statistic to determine if the smaller model is better. We can find its exact distribution. Remember that \n",
    "$$\\frac{\\|Y-P_0Y\\|^2}{\\|Y-PY\\|^2} = 1- \\frac{\\|Y-P_0Y\\|^2}{\\|Y-PY\\|^2}$$\n",
    "since $(Y-PY)^T(PY-P_0Y) = 0$. \n",
    "\n",
    "Now $Y^T(I-P)^T(I-P)Y=Y^T(I-P)Y$ and $\\|PY-P_0Y\\|^2 =Y^T(P-P_0)Y$ are independent $\\sigma^2\\chi^2_{n-p}$ and $\\sigma^2\\chi^2_{p-p_0}$ random variables under $H_0$ by Cochran's theorem. So,\n",
    "$$F=\\frac{\\frac{1}{p-p_0}\\|PY-P_0Y\\|^2}{\\frac{1}{n-p}\\|Y-PY\\|^2}\\sim F_{p-p_0, n-p}$$\n",
    "So, we reject $H_0$ if $F$ is larger than the upper $\\alpha$-point of this distribution. \n",
    "In fact, an extension of Cochran's theorem can be used to show that under $H_1$, $Y^T(I-P)Y$ and $Y^T(P-P_0)Y$ are still independent with $Y^T(I-P)Y\\sim \\sigma^2\\chi^2_{n-p}$ and $Y^T(P-P_0)Y$ are still independent with $Y^T(I-P)Y\\sim\\sigma^2\\chi^2_{n-p}$ and $Y^T(P-P_0)Y \\sim \\sigma^2\\chi^2_{p-p_0}$.\n",
    "\n",
    "\n",
    "Here $\\chi^2_n(\\lambda)$ denotes a nonconstant $\\chi^2$ distribution with $n$ degrees of freedom and a noncentraility parameter $\\lambda$. Note well, if $Z_1,\\ldots, Z_n$ are independent with $Z_i\\sim N(\\mu_i, 1)$,then $\\sum^n_{i=1}Z_i^2 \\sim \\chi^2_n(\\lambda)$ where $\\lambda = \\sum^n_{i=1}\\mu_i^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4\n",
    "\n",
    "Suppose that $Y_n=1$ with probability $\\frac{1}{n}$ and $0$ otherwise and that they are all independent. For any $\\epsilon>0$, then $P(|Y_n|>\\epsilon)\\le \\frac{1}{n}\\rightarrow 0$ as $n\\rightarrow \\infty$. So, $Y_n\\rightarrow 0$ in probability. \n",
    "\n",
    "But, **Borel Cantelli** supremum of this sequence is $1$ with probability $1$, so it doesn't converge almost surely. \n",
    "\n",
    "* **Borel Cantelli** Lemma 1: If $A_n$ is a sequence of events with $\\sum^\\infty_{n=1}P(A_n)<\\infty$, then $P(A_n)=0$ infinitely often. \n",
    "\n",
    "\n",
    "* **Borel Cantelli** Lemma 2: If $A_n$ is a sequence of individual events with $\\sum^\\infty_{i=1}P(A_n)=\\infty$, then $P(A_n)=1$ infinitely often.\n",
    "\n",
    "\n",
    "### Cochran's Theorem\n",
    "Recall that $Y\\sim N(0,\\sigma^2I)$. $A_1,\\ldots, A_u$ are symmetric $n\\times n$ matrices with $\\text{rank}(A_i)=r_i$ and $A_1+\\ldots +A_k=I$ and $r_1+\\ldots+r_kA=n$. Show that $Y^TA_iY\\sim \\sigma^2\\chi^2_{r_i}$ using a diagonalization argument. \n",
    "\n",
    "We can find an orthogonal matrix $Q$ such that $Q^TA_iQ=D_{r_i}$ where $D_{r_i}$ is a diagonal with non-zero entries in the first $r_i$ diagonal elements and $0$s on the other diagonal elements and zeros elsewhere. \n",
    "\n",
    "Consider $Q^T(I-A_i)Q=I-Q^TAQ$, which is diagonal with the last $n-r_i$ diagonal entries equal to $1$. So, $\\text{rank}(Q^T(I-A_i)Q)\\ge n-r_i$, but $Q^T(I-A_i)Q=\\sum_{j\\not = i} Q^tA_jQ$, so $\\text{rank}(Q^T(I-A_i)Q)\\le \\sum_{j\\not = i }\\text{rank}(Q^TA_jQ) = n-r_i$. So, the first $r_i$ diagonal entries of $Q^TA_iQ$ are $1$. \n",
    "\n",
    "Now, set $Z=Q^TY$, so $Z\\sim N(0,\\sigma^2I)$. Then, $Y^TA_iY$ equals $Z^TQ^TA_iQZ = \\sum^{r_i}_{i=1}Z_i^2\\sim \\sigma^2\\chi_r^2$.\n",
    "\n",
    "Now, we need to show the independence of $Y^TA_iY$ for $i=1,\\ldots, k$. Use MGF argument. First show $2$ properties of $A_1,\\ldots, A_k$. Claim that those $A_i$ represented orthogonal projections that project onto orthogonal subspaces. Note that \n",
    "$$\n",
    "A_i = Q\\left(\\begin{array}{cc}\n",
    "I_{r_i} & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "\\end{array}\\right)Q^T\n",
    "Q\\left(\\begin{array}{cc}\n",
    "I_{r_i} & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "\\end{array}\\right)Q^T = \n",
    "Q\\left(\\begin{array}{cc}\n",
    "I_{r_i} & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "\\end{array}\\right)Q^T\n",
    "=A_i$$\n",
    "So, $A_i$ represents an orthogonal projection. \n",
    "\n",
    "If $i\\not = j$, then \n",
    "$$I-Q^TA_iQ-Q^TA_jQ = \\sum_{\\ell \\not = i,j} Q^TA_\\ell Q$$ is non-negative definite because $y^TQ^TA_\\ell Qy = y^TQ^TA_\\ell^TA_\\ell Qy = \\|A_\\ell Q y\\|^2 > 0$. Fix $y\\in \\mathbb{R}^n$ and set $z=Q^TA_iQy$, so last $n-r_i$ components of $z$ are $0$. Thus, \n",
    "$$0\\le z^T(I-Q^TA_iQ-Q^TA_jQ)z = -z^TQ^TA_jQz=-\\|A_jQz\\|^2 \\rightarrow A_j Qz = 0$$,\n",
    "so $A_jQQ^TA_iQy=A_jA_iQy = 0$ for all $y$. So, $A_jA_i=0$ because $Q$ is invertible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lecture 5\n",
    "\n",
    "Recall statement of Cochran's theorem. Think of decomposition \n",
    "$$I=P_0+(P-P_0)+(I-P)$$\n",
    "$P-P_0\\in U\\cap U_0^\\perp$ and $I-P \\in U^\\perp$. Last time, we showed $A_i^2=A_i$ and $A_iA_j=0$ for all $i\\not = j$. So, $A_1,\\ldots, A_k$ represent orthogonal projections onto orthogonal subspaces. \n",
    "\n",
    "Let $\\mu(t_1,\\ldots, t_k)$ denote the MGF of $\\frac{1}{\\sigma^2}(Y^TA_1Y,\\ldots, Y^TA_uY)$ and let $M_i(T)$ denote the MGF of $\\frac{1}{\\sigma^2}Y^TA_iY$. We note that for sufficiently small $|t_1|,\\ldots, |t_k|$, the matrices $I-2t_1A_1-\\ldots - 2t_kA_k$ and each $I-2t_iA_i$ are positive definite. (Consider $Y^T(I-2t_iA_i)Y$)\n",
    "\n",
    "FOr such $|t_1|,\\ldots, |t_k|$, \n",
    "$$\n",
    "\\begin{split}\n",
    "\\mu(t_1,\\ldots, t_k) \\\\\n",
    "=& \\int_{\\mathbb{R}^n}\\frac{1}{(2\\pi \\sigma^2)^{n/2}}e^{-\\frac{1}{2\\sigma^2}y^Ty}e^{\\frac{1}{\\sigma^2}(t_1y^TA_1y+\\ldots +t_ky^TA_ky^T)}dy\\\\\n",
    "= & \\int_{\\mathbb{R}^n}\\frac{1}{(2\\pi \\sigma^2)^{n/2}} e^{-\\frac{1}{2\\sigma^2}(y^T(I-2t_1A_1-\\ldots- 2t_kA_k)y)}dy\\\\\n",
    "=& (\\det(I-2t_1A_1-\\ldots - 2t_kA_k))^{-1/2}\\\\\n",
    "= &(\\det(I-2t_1A_1)\\ldots \\det(I-2t_kA_k))^{-1/2}\\\\\n",
    "=& \\prod^k_{i=1}\\det(I-2t_iA_i)^{-1/2}\\\\\n",
    "= &\\mu_1(t_1)\\ldots \\mu_k(t_k)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood Theory\n",
    "### Definition and Basic Properties\n",
    "In this chaper, we study parametricmodels for the distribution of a random vector $(Y_1,\\ldots, Y_n)^T$ taking values in $E$. Let $\\Theta$ be a subset of $\\mathbb{R}^d$ called the parameter space. Let $f_Y:E\\times \\Theta \\rightarrow [0,\\infty)$ be such that for each $\\theta\\in\\Theta$, $f_y(\\cdot, \\theta)$ is the density of a probability measure with respect to a $\\sigma$-finite measure $\\mu$. \n",
    "\n",
    "The **likelihood function** of $\\theta$ is $L_Y^{(\\theta)}(\\theta; y)=L(\\theta;y)=L(\\theta)= f_Y(y;\\theta)$ (function of $\\theta$ for fixed $y$). Often, we work with log-likelihood $\\ell_y^{(\\theta)}=\\ell(\\theta;y)=\\ell(\\theta)=\\log _Y(y;\\theta)$. \n",
    "\n",
    "Asymptotic theory: approximate log-likelihood function as a quadratic function ner MLE. \n",
    "\n",
    "Sometimes, we consider the log-likelihood as a random variable: $\\ell^{(\\theta)}(\\theta; y)(\\omega)=\\ell^{(\\theta)}(\\theta;y(\\omega))$. The likelihood function compares the relative plausibility of different parameter values $r$ is defined up to a multiplicative constants. Similarly, log-likelihood is defined only up to an additive constant. So, normally, we add a constant so that $\\ell(\\hat\\theta_{MLE})=0$. \n",
    "\n",
    "Any value $\\hat \\theta = \\hat \\theta(Y)\\in\\Theta$ is called a {\\em maximum likelihood estimator}. Now suppse $\\ell(\\theta;y)$ is differentiable in $\\theta$ for each $y\\in E$. The score function is $u(\\theta;y) = \\nabla_\\theta \\ell(\\theta,y)$ where $\\nabla_\\theta = (\\frac{\\partial}{\\partial \\theta_1},\\ldots, \\frac{\\partial}{\\partial \\theta_T})^T$. Note $0=u(\\hat \\theta) \\approx u(\\theta_0)-(\\hat \\theta-\\theta_0)j(\\theta_0)$. \n",
    "\n",
    "##### Dominated Converengence Condition\n",
    "Let $\\theta_0$ be an interior point of $\\Theta$ and suppose $\\exists \\delta >0$ and a $\\mu$-integrable function $g$ such that $\\sup_{\\theta:\\|\\theta-\\theta_0\\|<\\delta}\\|u(\\theta;y)\\|\\le g(y)$. Then, $\\mathbb{E}_{\\theta_0}(u(\\theta_0))=0$. \n",
    "\n",
    "##### Proof.\n",
    "For $r=1,\\ldots, d$, $\\mathbb{E}_{\\theta_0}(u_r(\\theta_0))=\\int_E\\frac{\\partial}{\\partial \\theta_r}\\log f_Y(y;\\theta) f_Y(y;\\theta)d\\mu(y) = \\int_E\\frac{\\partial}{\\partial \\theta_r}f_Y(y;\\theta)d\\mu(y)$. \n",
    "The DCC lets us move the derivative outside of the integral, which means that it is equal to \n",
    "$$\\frac{\\partial}{\\partial \\theta_r}\\int_E f_Y(y;\\theta)d\\mu(y)= \\frac{\\partial}{\\partial \\theta_r} 1 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 6\n",
    "\n",
    "Under slightly stronger conditions, for $r,s=1,\\ldots, d$, $\\text{Cov}_{\\theta}(U_r(\\theta),U_s(\\theta)) = \\mathbb{E}_\\theta(\\frac{\\partial}{\\partial \\theta_r} \\ell(\\theta; y)\\frac{\\partial}{\\partial \\theta_s}\\ell(\\theta;y)) = -\\mathbb{E}_\\theta (\\frac{\\partial^2}{\\partial \\theta_r\\partial \\theta_s}\\ell(\\theta;y))$.\n",
    "\n",
    "Thus, the covariance matrix of $u(\\theta)$ is $i(\\theta)= -\\mathbb{E}_\\theta(\\nabla_\\theta\\nabla_\\theta^T\\ell(\\theta))$ and is called the fisher information matrix $j(\\theta)=-\\nabla_\\theta\\nabla_\\theta\\ell(\\theta)$ is called the observed information matrix. \n",
    "\n",
    "Obs: The likelihood function is parametrization invariant. If $\\psi=\\psi(\\theta)$ is a smooth reparametrization with inverse $\\theta=\\theta(\\psi)$, then $\\psi$ (in the $\\psi$-parametrization) and $\\theta$ (in the $\\theta$-parametrization) index the same distribution. So, $L^{(\\psi)}(\\psi)=L^{(\\theta)}(\\theta)$. In particular, the MLE's of both identify the same distribution, so $\\hat \\psi = \\psi(\\hat \\theta)$. This is called the {\\em equivariance property} of the MLE> \n",
    "\n",
    "The score function and the Fisher information matrix do depend on parametrization. For $a=1,\\ldots, d$:\n",
    "$$U_a^{(\\psi)}(\\psi) = \\frac{\\partial \\ell^{(\\psi)}(\\psi)}{\\partial \\psi_a} = \\frac{\\partial}{\\partial \\psi_a} \\ell^\\theta(\\theta(\\psi)$$\n",
    "$$=\\sum^d_{r=1}\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_r}(\\theta(\\psi)\\frac{\\partial \\theta_r}{\\partial \\psi_a} = \\sum^d_{r=1}U_r^{(\\theta)}(\\theta(r))\\frac{\\partial \\theta_r}{\\partial \\psi_a}$$\n",
    "\n",
    "In other words, $U^{(\\psi)}(\\psi) = \\left(\\frac{\\partial \\theta}{\\partial \\psi}\\right)^TU^{(\\theta)}(\\theta(\\psi))$ where the Jacobian matrix $\\left(\\frac{\\partial \\theta}{\\partial \\psi}\\right)$ has $(r,a)$th component $\\frac{\\partial_r}{\\partial \\psi_k}$. Similarly $i^{(\\psi)}(\\psi) = \\left(\\frac{\\partial \\theta}{\\partial \\psi}\\right)^Ti^{(\\theta)}(\\theta(\\psi))\\left(\\frac{\\partial \\theta}{\\partial \\psi}\\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asymptotic property of MLE\n",
    "In this section, we assume $Y=(Y_1,\\ldots, Y_n)^T$ has iid components. Let $\\theta_0$ denote the true value of $\\theta$ and impose the following additional restrictions on the model from the previous section.\n",
    "\n",
    "* [1.] $\\Theta$ should be closed and bounded.\n",
    "* [2.] For each $y\\in E$, the likelihood $L(\\theta;y)$ is a continuous function of $\\theta$.\n",
    "* [3.] For each $y\\in E$, the maximizers of $L(\\theta;y)$ (which exist by the previous conditions) is allowed to be unique. We denote it $\\hat \\theta_n$. Lemma A3 of Potscher and Prucha (1997) show that $\\hat \\theta_n$ is a measurable function of $Y$. \n",
    "* [4.] The model is identifiable if $\\theta\\not = \\theta'$, then $\\exists$ some $A\\in \\Sigma$ such that $\\int_A f_Y(y;\\theta)d\\mu(y) \\not = \\int_Af_Y(y;\\theta')d\\mu(y)$. \n",
    "\n",
    "Here is an example of a on-identifiable model. Let $y_{ij} =\\mu+\\theta_i+\\epsilon_{ij}$. This is not identifiable because you can add $c$ to each $\\theta_i$ and remove $c$ from $\\mu$. \n",
    "\n",
    "* [5.] $\\mathbb{E}_{\\theta_0}(\\sup_{\\theta\\in\\Theta}|\\log f(y;\\theta)|) <\\infty$ where $f(\\cdot;\\theta)=f_{Y_1}(\\cdot; \\theta)$.\n",
    "\n",
    "Notation: $\\bar \\ell_n(\\theta) = \\frac{1}{n} \\sum^n_{i=1} \\log f_y(y_i;\\theta)$ for the normalized (empirical) log-likelihood and $\\bar \\ell(\\theta)=\\mathbb{E}_{\\theta_0}\\log (y_1;\\theta)$. \n",
    "\n",
    "Intuition for why $\\hat \\theta$ should be consistent: $\\hat \\theta$ maximizes $\\bar \\ell_n(\\theta)$, so it should maximize $\\mathbb{E}_{\\theta_0}(\\log (y;\\theta))$, since $\\bar \\ell_n(\\theta)\\rightarrow \\ell(\\theta)$ as $n\\rightarrow \\infty$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Theorem:\n",
    "Under conditions 1-5, $\\hat \\theta_n$ is consistent, that is $\\hat \\theta_n\\rightarrow \\theta_0$ in probability as $n\\rightarrow \\infty$. \n",
    "##### Proof.\n",
    "By Jensen's inequality (and condition 5), we have \n",
    "$$\\bar \\ell(\\theta)-\\bar \\ell(\\theta_0) = \\mathbb{E}_{\\theta_0}(\\log \\frac{f(y_1;\\theta)}{f(y_1;\\theta_0)})$$\n",
    "$$\\le \\log \\mathbb{E}_{\\theta_0}\\frac{f(y;\\theta)}{f(y;\\theta_0)}=\\log(1)=0$$\n",
    "We have equality if and only if $P_{\\theta_0}f(y_1;\\theta) = f(Y_1;\\theta_0) = 1$. By condition 4, this implies that $\\theta=\\theta_0$. So, $\\theta_0$ is unique maximizer of $\\bar \\ell(\\cdot)$. \n",
    "\n",
    "\n",
    "Note that by condition 5, the function $\\bar \\ell(\\theta)$ is continuous ( an application of dominated convergence theorem), so it contains its bounds on any closed bounded set. In particular, given $\\epsilon>0$, $\\exists \\delta>0$ such that $\\bar\\ell(\\theta_0)-\\bar\\ell(\\theta)\\ge \\delta$ whenever $\\|\\hat \\theta_n-\\theta_0\\|\\ge \\epsilon$. \n",
    "\n",
    "Under condition 5, we can prove that $\\sup_{\\theta\\in\\Theta}|\\hat\\ell_n(\\theta)-\\bar\\ell(\\theta)|\\rightarrow 0$ in probability. This is called a **uniform law of large numbers**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 7\n",
    "Recap: $\\bar \\ell_n(\\theta) = \\frac{1}{n}\\sum^n_{i=1} \\log f(Y_i;\\theta)$, $\\bar \\ell(\\theta) = \\mathbb{E}_{\\theta_0}(\\log f(Y_i;\\theta))$. We proved that $\\forall \\epsilon$, $\\exists \\delta >0$ such that $\\bar  \\ell(\\theta_0)-\\bar\\ell(\\theta) \\ge \\delta$ whenever $\\|\\theta-\\theta_0\\|\\ge \\epsilon$. \n",
    "\n",
    "It follows that $P_{\\theta_0}(\\|\\hat \\theta_n-\\theta_0\\| >\\epsilon) \\le P_{\\theta_0}(\\bar \\ell (\\theta_0)-\\bar \\ell(\\hat \\theta_n) \\ge \\delta) = P_{\\theta_0}(\\bar \\ell_n(\\hat \\theta_n)-\\bar \\ell(\\hat \\theta_n)+\\bar \\ell(\\theta_0)-\\bar \\ell_n(\\hat \\theta_n)\\ge \\delta)$\n",
    "$$\\le P(\\sup_{\\theta\\in\\Theta}|\\bar \\ell_n(\\theta)-\\bar \\ell(\\theta)|+\\bar \\ell(\\theta_0)-\\bar \\ell_n(\\theta_0)\\ge \\delta)$$\n",
    "$$\\le P(\\sup_{\\theta\\in\\Theta}|\\bar \\ell_n(\\theta)-\\bar \\ell(\\theta)|\\ge \\frac{\\delta}{2})+P(|\\bar \\ell_n(\\theta_0)-\\bar \\ell(\\theta_0)|\\ge \\frac{\\delta}{2})$$\n",
    "\n",
    "The first goes to $0$ by the uniform law of large numbers and the second goes to $0$ by the weak law of large numbers. \n",
    "\n",
    "For Aymptotic normality, we require some further conditions:\n",
    "\n",
    "* [6.] $\\theta_0$ is in the interior of $\\Theta$. (We want to use Taylor expansions).\n",
    "\n",
    "* [7.] For each $y\\in E$, the likelihood $L(\\theta;y)$ is twice continuously differentiable in $\\theta$; moreover, $\\bar \\ell(\\cdot)$ is twice continuously differentiable. \n",
    "\n",
    "* [8.] \n",
    " * [a.] $\\mathbb{E}_{\\theta_0}(\\sup_{\\theta\\in\\Theta}\\|U^{(1)}(\\theta)\\|^2)<\\infty$ where $U^{(1)}(\\theta)=\\nabla_\\theta \\log f(y_1;\\theta)$\n",
    " * [b.] $\\mathbb{E}_{\\theta_0}(\\sup_{\\theta\\in\\Theta}\\|j^{(1)}(\\theta)\\|^2)<\\infty$ where $j^{(1)}(\\theta)=-\\nabla_\\theta\\nabla_\\theta\\log(f(y_1;\\theta))$ and where $\\||A|\\|^2 = \\sum_{j=1}^d \\sum^d_{k=1} A_{jk}^2$.\n",
    "\n",
    "\n",
    "* [9.] Under a weaker version of 8a, we proved $\\mathbb{E}_{\\theta_0}(U^{(1)}(\\theta_0))=0$ (proposition 1) and under conditions 8a and 8b,\n",
    "\n",
    "$$ i^{(1)}(\\theta_0) = \\mathbb{E}_{\\theta_0}(\\nabla_\\theta\\nabla_\\theta^T\\log f(y_1;\\theta)) = \\mathbb{E}_{\\theta_0}(U^{(1)}(\\theta_0)U^{(1)}(\\theta_0)) $$\n",
    "We assume this matrix is positive definite. It is always non-negative definite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem \n",
    "Under conditions 1-8, $n^{1/2}(\\hat \\theta_n-\\theta)\\rightarrow N_d(0,i^{(1)}(\\theta_0)^{-1})$ in distribution. \n",
    "\n",
    "#### Proof\n",
    "Let $A_n$ denote the event that ($\\hat \\theta_n$ is an interior point of $\\Theta$). By theorem 2 and condition 6, $P(A_n)\\rightarrow 1$ as $n\\rightarrow \\infty$. \n",
    "\n",
    "Let $\\bar U(\\theta) = \\frac{1}{n}\\sum^n_{i=1}\\nabla_\\theta \\log (y_i;\\theta)$ and $\\bar j(\\theta) = \\frac{1}{n}\\sum^n_{i=1} \\nabla_\\theta\\nabla_\\theta^T \\log(y_i;\\theta)$.\n",
    "\n",
    "By condition 7:\n",
    "\n",
    "$$\n",
    "0 = \\bar U(\\hat \\theta_n)\\mathbb{1}_{A_n} = \\bar U(\\theta_0)\\mathbb{1}_{A_n}-\\bar j(\\tilde\\theta_n)^T(\\hat \\theta_n-\\theta_0)\\mathbb{1}_{A_n} \n",
    "$$\n",
    "where $\\tilde \\theta_n$ lies on the line segment between $\\theta_0$ and $\\hat \\theta_n$, that is $\\tilde \\theta_n \\in t\\theta_0+(1-t)\\hat \\theta_n; t\\in [0,1]$. \n",
    "\n",
    "We will argue that we can replace $\\bar{j}$ with $i$ asymptotically. Now\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    " \\|\\bar j(\\tilde \\theta_n)\\mathbf{1}_{A_n}-i^{(1)}(\\theta_0)\\|\\le & \\|\\bar j(\\tilde \\theta_n)\\mathbf{1}_{A_n}-i^{(1)}(\\tilde \\theta_n)\\mathbf{1}_{A_n}\\|\\\\ + &\\|i^{(1)}(\\tilde \\theta_n)\\mathbf{1}_{A_n}-i^{(1)}(\\theta_0)\\mathbf{1}_{A_n}\\|\\\\ + & \\|i^{(1)}(\\theta_0)\\mathbf{1}_{A_n}-i^{(1)}(\\theta_0)\\|\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Note $\\|\\bar{j}(\\tilde \\theta_n)\\mathbf{1}_{A_n}-i^{(1)}(\\tilde \\theta_n)\\mathbf{1}_{A_n}\\|\\le \\sup_{\\theta\\in\\Theta}\\||\\bar j (\\theta)-i^{(1)}(\\theta_0)|\\|$. Taking out $\\mathbf{1}_{A_n}$ only makes it bigger. Also note,\n",
    "$$\\|i^{(1)}(\\theta_0)\\mathbf{1}_{A_n}-i^{(1)}(\\theta_0)\\| = \\|i^{(1)}(\\theta_0)\\|\\mathbf{1}_{A_n^C}$$\n",
    "Under 8b, the first term converges in probability to $0$ by Uniform Law of Large Numbers. The second term converges in probability to $0$ because $\\|\\tilde\\theta_n-\\theta_0\\|\\le \\|\\hat \\theta_n-\\theta_0\\|\\rightarrow 0$ by theorem 2 and because $i^{(1)}$ is continuous by condition 7 and Dominated Convergence Theorem.  Finally, the third term converges to $0$ in probability because $P(A_n^C)\\rightarrow 0$ as $n\\rightarrow \\infty$. \n",
    "\n",
    "Thus, if $B_n$ denotes the event $A_n\\cap \\text{($\\bar j(\\tilde \\theta_n)$ is positive definite)}$, then $P(B_n)\\rightarrow 1$ by condition 9. It follows from equation ?? that $n^{1/2}(\\hat \\theta_n-\\theta)\\mathbf{1}_{B_n}=\\bar j(\\tilde \\theta_n)^{-1}\\mathbf{1}_{B_n}\\cdot n^{1/2}\\bar U(\\theta_0)$. $\\bar j(\\tilde \\theta_n)^{-1}\\mathbf{1}_{B_n}\\rightarrow i^{(1)}(\\theta_0)^{-1}$ in probability and $ n^{1/2}\\bar U(\\theta_0)\\rightarrow N(0,i^{(1)}(\\theta_0))$ in distribution. By Slutsky's theorem, the whole thing goes to $N_d(0,i^{(1)}(\\theta_0)^{-1})$. \n",
    "\n",
    "We need to show that removing $\\mathbf{1}_{B_n}$ doesn't change the limit. Finally, if $g$ is a continuous function bounded by $c$, then $\\mathbb{E}_{\\theta_0}(g(n^{1/2}(\\hat \\theta_n-\\theta)-n^{1/2}(\\hat \\theta_n-\\theta_0)\\mathbf{1}_{B_n})) \\le C\\cdot P(B_n^C)\\rightarrow 0$ as $n\\rightarrow\\infty$. So, $n^{1/2}(\\hat \\theta_n-\\theta_0)\\mathbf{1}_{B_n}\\rightarrow n^{1/2}(\\hat \\theta_n-\\theta_0)$ in probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result can be generalized to much more complicated models (without iid components) and indeed this is where its most useful. It forms the basis for standard hypothesis tests and confidence sets based on wald, score and likelihood ratio statistics. For instance, the confidence set for $(1-\\alpha)$-level coverage based on the Wald statistic ($w_w(\\theta_0)=(\\hat \\theta_n-\\theta_0)^T(\\theta_0)(\\hat \\theta_n-\\theta_0)$) is given by $\\{\\theta_0\\in\\Theta \\text{ such that } w_w(\\theta_0)\\subset \\chi^2_d(\\alpha)\\}$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
