{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression (Hoerl and Kennard, 1970)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the linear model with an intercept term $ Y=\\beta_{0}+X\\beta+\\epsilon$ .When some of the columns of X\n",
    "  are nearly collinear (as is likely when $p$\n",
    "  is nearly as large as n), the determinant of $X^{T}X $\n",
    " , which is the square of the volume of the parallelepiped whose edges are the columns of $X$\n",
    " , is nearly 0. Thus, since a determinant is the product of the eigenvalues, at least one of the eigenvalues of $ \\left(X^{T}X\\right)^{-1} $ very large. But  $ \\hat{\\beta}\\sim N_{p}\\left(\\beta,\\,\\sigma^{2}\\left(X^{T}X\\right)^{-1}\\right) $\n",
    " , at least when the columns of X\n",
    "  are orthogonal to $ (1,1,1,...,1,1)^{T} $\n",
    " , and the sum of the eigenvalues is the trace, so at least one of the components of $\\hat{\\beta}$\n",
    "  has very large variance. We therefore seek to shrink $\\hat{\\beta}$\n",
    "  towards the origin. This will introduce a bias, but we hope it will be more than compensated by a reduction in variance. \n",
    "\n",
    "For fixed $\\lambda>0 $\n",
    " , the ridge regression estimator is  $ \\hat{\\beta}_{\\lambda}^{R} $\n",
    " , where $ \\left(\\hat{\\beta}_{0},\\hat{\\beta}_{\\lambda}^{R}\\right) $\n",
    "  minimises $$ Q_{2}(\\beta_{0},\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-\\beta_{0}-\\sum_{j=1}^{p}x_{ij}\\beta_{j}\\right)^{2}+\\lambda\\sum_{j=1}^{p}\\beta_{j}^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In that case, $$\\hat{\\beta}_{\\lambda}^{R}=(X^{T}X+\\lambda I)^{-1}X^{T}Y $$ (see example sheet)\n",
    "\n",
    "Observe that $ \\hat{\\beta}_{\\lambda}^{R}$ \n",
    "  also minimises $$ \\sum_{i=1}^{n}\\left(Y_{i}-\\sum_{j=1}^{p}x_{ij}\\beta_{j}\\right)^{2} $$\n",
    "  subject to  $ \\sum_{j=1}^{p}\\beta_{j}^{2}\\leq s $\n",
    "  where $ s=s(\\lambda) $ is a bijective function. It can also be motivated in a Bayesian way as the maximum a posteriori (MAP) and posterior mean estimators of $\\beta$ under a $N_{p}\\left(0,\\,\\frac{1}{\\lambda}I\\right)$\n",
    "  prior. \n",
    "\n",
    "In order to study the Mean Squared Error (MSE) properties of $ \\hat{\\beta}_{\\lambda}^{R} $\n",
    " , we define \n",
    " $V=\\left(I+\\lambda\\left(X^{T}X\\right)^{-1}\\right)^{-1} $ and $W=\\left(X^{T}X+\\lambda I\\right)^{-1}$\n",
    " , \n",
    " \n",
    " so that $$ \\hat{\\beta}_{\\lambda}^{R}=\\left(X^{T}X+\\lambda I\\right)^{-1}X^{T}Y=WX^{T}Y=V\\hat{\\beta} $$\n",
    " \n",
    "and  $$ V-I=\\left(I+\\lambda(X^{T}X)^{-1}\\right)^{-1}-I=WX^{T}X-I=W\\left(X^{T}X-W^{-1}\\right)=-\\lambda W $$\n",
    "\n",
    "Assume that $X$ has full rank p (in particular, $ p\\leq n $\n",
    " ), and write $\\mu_{1}\\geq...\\geq\\mu_{p}>0 $\n",
    "  for the eigenvalues of  $X^{T}X$\n",
    " . Let P\n",
    "  be an orthogonal matrix such that $P^{T}X^{T}XP=D\\equiv\\mbox{diag}(\\mu_{1},...,\\mu_{p}) $\n",
    " . Then $$ \\det\\left(W-\\frac{1}{\\mu_{j}+\\lambda}I\\right)\t=\t\\det\\left(\\left(PDP^{T}+\\lambda I\\right)^{-1}-\\frac{1}{\\mu_{i}+\\lambda}I\\right)\n",
    "\t=\t\\det\\left(P(D+\\lambda I)^{-1}P^{T}-\\frac{1}{\\mu_{i}+\\lambda}PP^{T}\\right)\n",
    "\t=\t\\det\\left((D+\\lambda I)^{-1}-\\frac{1}{\\mu_{i}+\\lambda}I\\right)\n",
    "\t=\t0 $$\n",
    " \n",
    "\n",
    "Thus the eigenvalues of $W$\n",
    "  are $ \\frac{1}{\\mu_{1}+\\lambda}\\leq...\\leq\\frac{1}{\\mu_{p}+\\lambda} $\n",
    " ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 1\n",
    "\n",
    "For sufficiently small $ \\lambda>0 $\n",
    " , we have $ MSE(\\hat{\\beta}_{\\lambda}^{R}) < MSE(\\hat{\\beta}) $\n",
    " . \n",
    "\n",
    "#### Proof: \n",
    "\n",
    "\n",
    "$$ \n",
    "MSE(\\hat{\\beta}_{\\lambda}^{R})\n",
    "  = \\mathbb{E}_{\\beta}\\left(\\left\\Vert \\hat{\\beta}_{\\lambda}-\\beta\\right\\Vert ^{2}\\right) $$\n",
    "  $$ =\\mathbb{E}_{\\beta}\\left(\\left\\Vert V\\hat{\\beta}-V\\beta+V\\beta-\\beta\\right\\Vert \\right)\n",
    "  =\t\\mathbb{E}_{\\beta}\\left(\\left\\Vert V\\left(\\hat{\\beta}-\\beta\\right)\\right\\Vert \\right)+\\beta^{T}(V-I)^{T}(V-I)\\beta\n",
    "  $$\n",
    "  $$=\t\\sigma^{2}\\mbox{Tr}\\left(V\\left(X^{T}X\\right)^{-1}V^{T}\\right)+\\lambda^{2}B^{T}W^{T}W\\beta\n",
    "=\t\\sigma^{2}\\mbox{Tr}\\left(\\left(X^{T}X\\right)^{-1}V^{T}V\\right)+\\lambda^{2}\\beta^{T}\\left(PDP^{T}-\\lambda I\\right)^{-1}\\left(PDP^{T}-\\lambda I\\right)^{-1}\\beta\n",
    "\t$$\n",
    "    \n",
    "$$ =\t\\sigma^{2}\\mbox{Tr}\\left(W(I-\\lambda W)\\right)+\\lambda^{2}\\beta^{T}P\\left(D-\\lambda I\\right)^{-2}P^{T}\\beta\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\t\\sigma^{2}\\sum_{j=1}^{p}\\frac{1}{\\mu_{j}+\\lambda}-\\lambda\\sigma^{2}\\sum_{j=1}^{p}\\frac{1}{\\left(\\mu_{j}+\\lambda\\right)^{2}}+\\lambda^{2}\\sum_{j=1}^{p}\\frac{\\alpha_{j}^{2}}{\\left(\\mu_{j}+\\lambda\\right)^{2}}\n",
    "$$\n",
    "\n",
    "$$=\t\\sigma^{2}\\sum_{j=1}^{p}\\frac{\\mu_{j}}{(\\mu_{j}+\\lambda)^{2}}+\\lambda^{2}\\sum_{j=1}^{p}\\frac{\\alpha_{j}^{2}}{\\left(\\mu_{j}+\\lambda\\right)^{2}} \n",
    "$$\n",
    " \n",
    "\n",
    "where $\\alpha=P^{T}\\beta$\n",
    " \n",
    "\n",
    "Thus the variance is a monotone decreasing function of $\\lambda$\n",
    " , while the squared bias is amonotone increasing function of $\\lambda$\n",
    " . We deduce that\n",
    "\n",
    "$$\\frac{d}{d\\lambda}\\mbox{MSE}\\left(\\hat{\\beta}_{\\lambda}^{R}\\right)\t=\t-2\\sigma^{2}\\sum_{j=1}^{p}\\frac{\\mu_{j}}{\\left(\\mu_{j}+\\lambda\\right)^{2}}+\\sum_{j=1}^{p}\\frac{\\left\\{ 2\\left(\\mu_{j}+\\lambda\\right)^{2}\\lambda-2\\lambda^{2}(\\mu_{j}+\\lambda)\\right\\} a_{j}^{2}}{\\left(\\mu_{j}+\\lambda\\right)^{4}}\n",
    "\t=\t-2\\sigma^{2}\\sum_{j=1}^{p}\\frac{\\mu_{j}}{\\left(\\mu_{j}+\\lambda\\right)^{3}}+2\\lambda\\sum_{j=1}^{p}\\frac{\\mu_{j}x_{j}^{2}}{\\left(\\mu_{j}+\\lambda\\right)^{3}}\n",
    "\t<\t0 $$\n",
    " for  $ 0\\leq\\lambda<\\frac{\\sigma^{2}}{\\alpha_{\\max}}$\n",
    " , where  $ \\alpha_{\\max}^{2}=\\max\\left(\\alpha_{1}^{2},...,\\alpha_{p}^{2}\\right)$\n",
    " \n",
    "\n",
    "Unfortunately, a poor choice of $\\lambda$\n",
    "  may substantially increase the MSE. In practice, $\\lambda$\n",
    "  is often chosen by V\n",
    " -fold cross validation. The idea is to randomly split the data into v groups (folds) of roughly the same size. For each $\\lambda$\n",
    "  on a grid of values, and for k=1,..,v\n",
    " , we compute $\\hat{\\beta}_{\\lambda,-k}^{R}$\n",
    " , the ridge regression estimator of $\\beta$\n",
    "  based on all of the data except the k-th fold. Writing $\\kappa(i)$\n",
    "  for the fold to which $\\left(X_{i},\\,Y_{i}\\right)$\n",
    "  belongs, we choose the value of $\\lambda$ that minimises\n",
    "\n",
    "$$CV(\\lambda)=\\sum_{i=1}^{n}\\left(Y_{i}-x_{i}^{T}\\hat{\\beta}_{\\lambda,\\,-k(i)}^{R}\\right)^{2} $$\n",
    " \n",
    "\n",
    "This is a fairly reliable way of choosing tuning parameters, although because we use the same data for both the fitting, and assessing the fit, the method can be prone to overfitting (choosing $\\lambda$ too small). Common choices of v include 5,10 or $n$, the last of these being known as 'leave-one-out' cross validation.\n",
    "\n",
    "$$ V(\\hat{\\beta}-\\beta)\\sim N_{p}\\left(0,\\,\\sigma^{2}V(X^{T}X)^{-1}V^{T}\\right)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 The LASSO (Tibshirani, 1996)\n",
    "\n",
    "The least absolute shrinkage and selection operator (LASSO) estimates\n",
    "$\\beta$ by $\\hat{\\beta}_{\\lambda}^{LASSO}$, where $\\left(\\beta_{0},\\hat{\\beta}_{\\lambda}^{LASSO}\\right)$\n",
    "minimizes\n",
    "\n",
    "$$\n",
    "Q(\\beta_{0},\\beta)=\\frac{1}{2}\\sum_{i=1}^{n}\\left(Y_{i}-\\beta_{0}-\\sum\\beta\\right)^{2}+\\lambda\\sum_{j=1}^{p}\\left|\\beta_{j}\\right|\n",
    "$$\n",
    "\n",
    "\n",
    "Like ridge regression, $\\hat{\\beta}_{\\lambda}^{LASSO}$ shrinks the\n",
    "least squares estimator (MLE) towards the origin. Crucially though,\n",
    "some of the components of $\\hat{\\beta}$ would be shrink to exactly\n",
    "zero, thus the LASSO performs simultaneous variable (model) selection\n",
    "and estimation. \n",
    "\n",
    "We way replace $Y_{i}$ with $Y_{i}-\\bar{Y}=Y_{i}-\\hat{\\beta}_{0}$\n",
    "so that with our new definition at $Y_{1},...Y_{n}$ the LASSO estimator\n",
    "$\\hat{\\beta}_{\\lambda}^{LASSO}$ minimizes\n",
    "\n",
    "$$\n",
    "Q_{1}(\\beta)=\\frac{1}{2}\\sum_{i=1}^{n}\\left(Y_{i}-\\sum_{j=1}^{p}x_{ij}\\beta_{j}\\right)^{2}+\\lambda\\sum_{j=1}^{n}\\left|\\beta_{j}\\right|\n",
    "$$\n",
    "\n",
    "\n",
    "Equivalently \n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda}^{LASSO}=\\arg\\min_{\\beta}\\frac{1}{2}\\left(Y_{i}-\\sum_{j=1}^{p}x_{ij}\\beta_{j}\\right)^{2}\n",
    "$$\n",
    " subject to $\\sum_{j=1}^{n}\\left|\\beta_{j}\\right|\\leq s$ or \n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda}^{LASSO}=\\arg\\min_{\\beta}\\sum_{j=1}^{n}\\left|\\beta_{j}\\right|\n",
    "$$\n",
    "\n",
    "\n",
    "subject to $\\frac{1}{2}\\sum_{i=1}^{n}\\left(Y_{i}-\\sum_{j=1}^{p}x_{ij}\\beta_{j}\\right)^{2}\\leq t$ \n",
    "\n",
    "TO see why some of the terms of $\\hat{\\beta}_{\\lambda}^{LASSO}$ will\n",
    "typically be zero. Think $X^{T}X=I$, in that case my original estimator\n",
    "is $\\hat{\\beta}=X^{T}Y$\n",
    "\n",
    "$$\n",
    "Q(\\beta)=\\frac{1}{2}\\left\\Vert Y-\\hat{Y}\\right\\Vert ^{2}+\\frac{1}{2}\\sum_{j=1}^{p}\\left(\\hat{\\beta_{j}}-\\beta_{j}\\right)^{2}+\\lambda\\sum_{j=1}^{p}\\left|\\beta_{j}\\right|\n",
    "$$\n",
    "\n",
    "\n",
    "(see example sheet) Since $\\hat{Y}$ does not depend on $\\beta$,\n",
    "we can minimize separately over each component and find\n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda}^{LASSO}=\\mbox{sgn}(\\hat{\\beta}_{j})(\\left|\\hat{\\beta}_{j}\\right|-\\lambda)\n",
    "$$\n",
    "\n",
    "\n",
    "In particular, those components of $\\hat{\\beta}$ that are smaller\n",
    "than $\\lambda$ will be shrunk to be exactly zero. The LASSO estimator\n",
    "in this special case is a **soft-thresholding estimator**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to understand the difference in behavior between the ridge regression estimator and the LASSO estimator is via geometry. The contours of the least squares objective function $\\|Y-X\\beta\\|^2$ are ellipsoids centered at $\\hat\\beta$. While contours of $\\sum^p_{i=1}\\beta_j^2$ are spheres centered at the origin and contours of $\\sum^p_{j=1} |\\beta_j|$ are diamonds centered at $0$. We therefore have the following picture when $p=2$: (See Charles's notes). \n",
    "z\n",
    "The important point to note is that the $\\ell_1$ ball, $\\{\\beta \\in \\R^p: \\sum^p_{j=1} |\\beta_j| \\le s\\}$ has corners,l where some of the components are zero, and it is likely that the least square criterion function will intersect the $\n",
    "\\ell_1$ ball at such a corner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Least angle regression (Efron, Hastie, Johnstone and Tibshirani, 2004)\n",
    "Unlike ridge regression, the LASSO estimator has no explicit closed form expression. However, $\\hat \\beta_{\\lambda}^{LASSO}$ minimizes a convex function of $\\beta$ subject to a less `less than or equal to' contraint or a convex function of $\\beta$: it is an ordinary convex program. Observe that the $\\ell_q$ constraint, $\\sum^p_{j=1} |\\beta_j|^q\\le s$, is a convex constraint iff $q\\ge 1$; on the other hand , the $\\ell_q$ ball has corners, so shrinks some components of $\\hat\\beta$ to zero, iff $q\\le 1$. Hence, among the class of $\\ell_q$ penalities, the $\\ell_1$ penalty, that is the LASSO, is a natural choice for many high dimensional regularization problems. \n",
    "\n",
    "The Least Angle Regression (LARS) algorithm is a model building algorithm which is of interest in its own right, and requires only a small modification to yield LASSO solutions. \n",
    "Recall that by location and scale transformations, we assume that \n",
    "$$\\sum^n_{i=1} y_i = 0, ~~\\sum^n_{i=1} x_{ij}  = 0 , ~ ~ \\sum^n_{i=1}x_{ij}^2 = 1$$\n",
    "for $j=1, \\ldots, p$. We assume that there are $\\min(n,p)$ linearly independent vectors among $\\vec x_1,\\ldots, \\vec x_n$ where $\\vec x_j = (x_{1j},\\ldots, x_{nj})^T$. \n",
    "\n",
    "Starting from $\\hat{\\vec{\\mu}}^0 =0$, the LARS algorithm builds up successive estimates $\\hat \\mu^k = X\\hat\\beta^k$ where $\\hat\\beta^k$ has $k$ non-zero entries. Consider first the case $p=2$, so $X=(\\vec{x}_1,\\vec{x}_2)$, and let $\\vec y_2 = X(X^TX)^{-1}X^Ty$ denote the orthogonal projection of $Y$ onto the subspace spanned by $\\vec x_1$ and $\\vec x_2$. We define, for a fixed vector $\\vec{\\mu} \\in \\mathbb{R}^n$, the {\\em vector of current correlations}\n",
    "$$\\vec c( \\vec{\\mu}) = X^T(\\vec{y}- \\vec{\\mu}) $$\n",
    "Note that forward stepwise selection would choose a larger value of $\\hat \\gamma\n",
    "^1$, so that\n",
    "$$\\hat\\mu^1 = \\vec y_1 = \\vec x_1(\\vec x_1^T \\vec x)\\vec x_1^T \\vec y = \\vec x_1\\vec x_1^T \\vec y$$\n",
    "At the next step, the LARS algorithm moves in the direction $\\vec \\mu^2$, which bisects the angle between $\\vec x_1$ and $\\vec x_2$. Thus, $\\hat \\mu^2 = \\hat \\mu^1+\\hat\\gamma^2 \\vec\\mu^2$. When $p=2$, we have $\\hat \\mu^2 = \\vec y_2$. However, when $p>2$, we would choose a smaller value of $\\hat \\gamma^2$, and then make a move in another direction. \n",
    "\n",
    "In general, starting from $\\hat \\mu^0$, suppose that $k-1$ iterations have been completed. Define the vector of current correlations\n",
    "$$\\vec c^k = \\vec c^k(\\hat \\mu^{k-1})$$\n",
    "and the set of {\\em active indices}\n",
    "$$\\mathcal{A}^k = \\{j\\in \\{1,\\ldots,j\\}:|c_j^k|=c^k\\}$$\n",
    "where $c^k = \\max\\{|c_j^k|:j\\in \\{1,\\ldots, j\\}\\}$. Define $s^k_j= \\text{sign}(c^k_j)$, and let $X^k$ denote the $n\\times k$ matrix whose $j$th column is $s^k_j\\vec x_j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap: LARS\n",
    "$\\hat\\mu^0 = 0$, and after $k-1$ iterations, have current correlations\n",
    "$$\\vec c^k = \\vec c^k(\\hat \\mu^{k-1}) = X^T(\\vec y - \\hat \\mu^{k-1})$$\n",
    "Active indices $\\{j:|c_j^k| = c^k\\}$, $c^k = \\max\\{|c_j^k|:j\\in \\{1,\\ldots, p\\}$ is max absolute correlation. Set $s_j^k = \\text{sign}(c_j^k)$ and $X_k$ is the $n\\times k$ matrix with $j$th column $s_j\\vec x_j$ for $j\\in A^k$. \n",
    "\n",
    "The ** equiangular unit vector ** is $\\vec u^k  = \\alpha^k X_k (X_k^TX_k)^{-1} \\vec 1_k$ where $\\alpha^k = (\\vec 1_k(X_k^TX_k)^{-1} \\vec 1_k)^{-1/2}$ is a normalization constant and $\\vec 1_k$ is a $k$-vector of ones. This is because \n",
    "$$X_k^T \\vec u^k = \\alpha^k \\vec 1_k$$\n",
    "We also define the {\\em inner product vector} $\\vec a^k = X^T\\vec u^k$. At the $k$th step, we move to \n",
    "$$\\vec \\mu^k = \\vec \\mu^k+\\hat \\gamma^k \\vec u^k\\text{,  }\\hat \\gamma^k = \\min^+_{j\\not \\in A^k}\\left(\\frac{c^k-c^k_j}{\\alpha^k-\\alpha^k_j},\\frac{c^k+c^k_j}{a^k+a^j}\\right)$$\n",
    "where $\\min^+$ denotes the fact that the minimum is taken only on those components, which are positive for each choice of $j$. This is the smallest positive choice of $\\gamma$ for which a new index enters the active set (see example sheet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LARS-LASSO relationship\n",
    "Suppose that $\\hat  \\beta^{\\text{LASSO}}$ is a LASSO solution (for some $\\lambda >0$) and $ \\hat\\mu^{\\text{LASSO}} = x \\hat\\beta^{\\text{LASSO}}$. It can be shown (see example sheet) that the sign of any non-zero coordinate $\\hat\\beta_j^{\\text{LASSO}}$ agrees with the sign of the current correlation $c_j^{\\text{LASSO}} = \\vec x^T_j(\\vec y - \\hat \\mu^{\\text{LASSO}})$. The LARS algorithm does not enforce this sign restriction, but it can easily be modified to do so. \n",
    "\n",
    "Suppose that $\\hat \\mu^k{-1} = X\\vec \\beta^{k-1}$ from the LARS algorithm agrees with the LASSO solution $\\hat \\mu^{\\text{LASSO}} = X\\hat\\beta^{\\text{LASSO}}$. We define $d_j^k = s_j^k(\\alpha^k(X^T_kX_k)^{-1}\\vec 1_k)_j$ for $j\\in A^k$ or $0$ otherwise. As we move in the positive $\\gamma$-direction along the LARS line $\\vec \\mu(\\gamma) = \\hat \\mu^{k-1} + \\gamma \\vec u^k$, and writing $\\vec \\mu(\\gamma ) = x\\vec \\beta(\\gamma)$, we see that $\\beta_j(\\gamma) = \\hat \\beta^{k-1}_j + \\gamma d^k_j$. The $j$th component therefore changes sign at $\\gamma_j^k = \\frac{-\\hat \\beta^{k-1}_j}{d^k_j}$, the first such occurence being at \n",
    "$$\\tilde \\gamma^k = \\min\\{\\gamma^k_j: \\gamma^k_j>0\\}$$\n",
    "If no $\\gamma^k_j$ is positive, set $\\tilde \\gamma^k = \\infty$. If $\\tilde \\gamma^k < \\hat \\gamma^k$ , then $\\vec \\beta(\\gamma)$ cannot be a LASSO solution for $\\gamma> \\tilde \\gamma^k$ because there is a sign violation in at least one component (observe that the signs of the current correlations do not change within a LARS step). \n",
    "\n",
    "\n",
    "** Assumption A ** At each iteration $k$, only one index $jk$ satisfies $\\gamma^k_{jk} = \\tilde\\gamma^k$.\n",
    "\n",
    "We can now modifty the LARS algorithm as follows: if $\\tilde\\gamma^k<\\hat\\gamma^k$, then we truncate the LARS step at $\\gamma = \\tilde \\gamma^k$, and remove the index $jk$ from the calculation of the next equiangular direction. Thus, we set $\\hat \\mu^k = \\hat \\mu^{k-1}+\\tilde\\gamma^k \\vec u^k$ and $A^{k+1} = A^k\\setminus \\{jk\\}$. \n",
    "\n",
    "##### Theorem: Under Assumption A, the above modification to the LARS algorithm yields all LASSO solutions. \\end{theorem}\n",
    "##### Proof  Look in Efron, Hastie, Johnston and Tibshirani (2004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other penalty functions\n",
    "A drawback of the LASSO estimator is that large estimated coefficients have a significant bias (downwards in absolute value). We therefore consider the more general penalized least squares problem of minimizing \n",
    "$$Q(\\beta) = \\frac{1}{2} \\|Y-X\\beta\\|^2 + \\sum^p_{j=i} p_\\lambda(|\\beta_j|)$$\n",
    "where $p_\\lambda$ is a nonnegative, nondecreasing function on $[0,\\infty)$ and is continuously differentiable. Here we define $p_\\lambda'(0) = p'_\\lambda(0^+) = \\lim_{t\\searrow 0}p_\\lambda'(t)$. Fan and Li (2001) proposed that a good penalty function should result in estimators with the following three properties:\n",
    "\\begin{enumerate}\n",
    "\\item[i.] Approximate unbiasedness: the resulting estimator should be approximately unbiased when the absolute value of the unknown true parameter is large. \n",
    "\\item[ii.] Sparsity: the resulting estimator should automatically set components of the least squares estimator that are small in absolute value. \n",
    "\n",
    "\\item[iii.] Continuity: the resulting estimator should be continuous in $\\hat \\beta$ to avoid instabilities in predictions. \n",
    "\\end{enumerate}\n",
    "\n",
    "In the following discussion, we assume that $t+p_\\lambda'(t)$ is strictly unimodal on the interval $[0,\\infty)$. Recall that in the orthonormal design case, we can minimize over each component of $\\beta$ separately, so supressing the $j$th subscript, it suffices to minimize the function \n",
    "$$R(\\beta) = \\frac{1}{2}(\\hat \\beta - \\beta)^2+p_\\lambda(|\\beta|)$$\n",
    "Since $R'(\\beta) = \\text{sign}(\\beta)(|\\beta|+p_\\lambda'(|\\beta|))-\\hat \\beta$, we see that when $p_\\lambda'(|\\beta|)=0$ for large $|\\beta|$, the resulting penalised least squares estimators is $\\hat \\beta$ for large $|\\hat \\beta|$. But when the absolute value of $\\beta$ is large, $|\\hat \\beta$ is large with high probability, so $p_\\lambda'(|\\beta|)=0$ for large $|\\beta|$ is a sufficient condition for approximate ubiasedness. \n",
    "\n",
    "When $|\\hat \\beta| <\\inf_{\\beta \\in \\R}(|\\beta|+p_\\lambda(|\\beta|)$, we see that $R'(\\beta)$ is positive for positive $\\beta$, and negative for negative $\\beta$, so $\\inf_{\\beta \\in \\R}(|\\beta|+p_\\lambda'(|\\beta|)) >0$ is a sufficient condition for the sparsity of the resulting estimator. \n",
    "\n",
    "When $|\\hat \\beta|> \\inf_{\\beta\\in\\R}(|\\beta|+p_\\lambda'(|\\beta|))$, we have the following picture. Thus, if there are multiple crossing points, then the resulting estimator is either zero or the crossing point after the minimum of $|\\beta|+p_\\lambda'(|\\beta|)$. On the other hand, if the minimum of $|\\beta|+p_\\lambda'(|\\beta|)$ is attained at zero, the resulting estimator is continuous in $\\hat \\beta$. \n",
    "\n",
    "The **Smoothly Chipped Absolute Deviation** (SCAD) penalty is defined by $p_\\lambda(0) = 0$ and \n",
    "$$p_\\lambda'(|\\beta|) = \\lambda \\mathbf{1}_{(|\\beta|\\le \\lambda)} + \\frac{(a\\lambda -|\\beta|)^+}{a-1}\\mathbf{1}_{(|\\beta|>\\lambda)}$$\n",
    "where $a>2$. The resulting SCAD estimator satisfies all three of the above requirements. See figure 2 for an image. The choice of $a=3.7$ has been recommended by a Bayesian argument. \n",
    "\n",
    "Note that any penalty function where the resulting estimator satisfies the three properties listed above cannot be convex. In fact, the SCAD penalty is concave on $[0,\\infty)$. This raises computational issues as well as issues to do with the existence and uniqueness of solutions. Zou and Li (2008) proposed {\\em local linear approximate} to compute the estimators. Based on the Taylor approximation\n",
    "$$p_\\lambda(|\\beta|)\\approx p_\\lambda(|\\beta^0|) + p'_\\lambda(|\\beta_0|)(|\\beta|-|\\beta^0|)$$\n",
    "Based on this approximation, we iteratively compute \n",
    "$$\\hat\\beta^{k+1} = \\arg \\min_\\beta \\frac{1}{2} \\|Y-X\\beta\\|^2 + \\sum^p_{j=i}p_\\lambda'(|\\beta_j^k|)|\\beta_j|$$\n",
    "Each iteration involves solving an ordinary complex program, so can be solved quickly. Remarkably, provided the initial estimator is reasonably good, it turns out that one step of the above procedure is as good as the fully iterative procedure. Studying the statistical properties of penalized least squares estimators is an active area of research. Finally, we remark that almost all of the above discussion can be extended beyond the linear model to more general likelihood models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
