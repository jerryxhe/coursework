{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression (Hoerl and Kennard, 1970)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the linear model with an intercept term $ Y=\\beta_{0}+X\\beta+\\epsilon$ .When some of the columns of X\n",
    "  are nearly collinear (as is likely when $p$\n",
    "  is nearly as large as n), the determinant of $X^{T}X $\n",
    " , which is the square of the volume of the parallelepiped whose edges are the columns of $X$\n",
    " , is nearly 0. Thus, since a determinant is the product of the eigenvalues, at least one of the eigenvalues of $ \\left(X^{T}X\\right)^{-1} $\n",
    " is very large. But  $$ \\hat{\\beta}\\sim N_{p}\\left(\\beta,\\,\\sigma^{2}\\left(X^{T}X\\right)^{-1}\\right) $$\n",
    " , at least when the columns of X\n",
    "  are orthogonal to $ (1,1,1,...,1,1)^{T} $\n",
    " , and the sum of the eigenvalues is the trace, so at least one of the components of $\\hat{\\beta}$\n",
    "  has very large variance. We therefore seek to shrink $\\hat{\\beta}$\n",
    "  towards the origin. This will introduce a bias, but we hope it will be more than compensated by a reduction in variance. \n",
    "\n",
    "For fixed $\\lambda>0 $\n",
    " , the ridge regression estimator is  $$ \\hat{\\beta}_{\\lambda}^{R} $$\n",
    " , where $$ \\left(\\hat{\\beta}_{0},\\hat{\\beta}_{\\lambda}^{R}\\right) $$\n",
    "  minimises $$ Q_{2}(\\beta_{0},\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-\\beta_{0}-\\sum_{j=1}^{p}x_{ij}\\beta_{j}\\right)^{2}+\\lambda\\sum_{j=1}^{p}\\beta_{j}^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In that case, $\\hat{\\beta}_{\\lambda}^{R}=(X^{T}X+\\lambda I)^{-1}X^{T}Y $ (see example sheet)\n",
    "\n",
    "Observe that $ \\hat{\\beta}_{\\lambda}^{R}$ \n",
    "  also minimises $$ \\sum_{i=1}^{n}\\left(Y_{i}-\\sum_{j=1}^{p}x_{ij}\\beta_{j}\\right)^{2} $$\n",
    "  subject to  $ \\sum_{j=1}^{p}\\beta_{j}^{2}\\leq s $\n",
    "  where $ s=s(\\lambda) $ is a bijective function. It can also be motivated in a Bayesian way as the maximum a posteriori (MAP) and posterior mean estimators of \\beta\n",
    "  under a N_{p}\\left(0,\\,\\frac{1}{\\lambda}I\\right)\n",
    "  prior. \n",
    "\n",
    "In order to study the Mean Squared Error (MSE) properties of $ \\hat{\\beta}_{\\lambda}^{R} $\n",
    " , we define $V=\\left(I+\\lambda\\left(X^{T}X\\right)^{-1}\\right)^{-1} $\n",
    "  and $W=\\left(X^{T}X+\\lambda I\\right)^{-1}$\n",
    " , so that $ \\hat{\\beta}_{\\lambda}^{R}=\\left(X^{T}X+\\lambda I\\right)^{-1}X^{T}Y=WX^{T}Y=V\\hat{\\beta} $\n",
    " \n",
    "\n",
    "and  $$ V-I=\\left(I+\\lambda(X^{T}X)^{-1}\\right)^{-1}-I=WX^{T}X-I=W\\left(X^{T}X-W^{-1}\\right)=-\\lambda W $$\n",
    "\n",
    "Assume that $X$\n",
    "  has full rank p (in particular, $ p\\leq n $\n",
    " ), and write $\\mu_{1}\\geq...\\geq\\mu_{p}>0 $\n",
    "  for the eigenvalues of  $X^{T}X$\n",
    " . Let P\n",
    "  be an orthogonal matrix such that $P^{T}X^{T}XP=D\\equiv\\mbox{diag}(\\mu_{1},...,\\mu_{p}) $\n",
    " . Then $$ \\det\\left(W-\\frac{1}{\\mu_{j}+\\lambda}I\\right)\t=\t\\det\\left(\\left(PDP^{T}+\\lambda I\\right)^{-1}-\\frac{1}{\\mu_{i}+\\lambda}I\\right)\n",
    "\t=\t\\det\\left(P(D+\\lambda I)^{-1}P^{T}-\\frac{1}{\\mu_{i}+\\lambda}PP^{T}\\right)\n",
    "\t=\t\\det\\left((D+\\lambda I)^{-1}-\\frac{1}{\\mu_{i}+\\lambda}I\\right)\n",
    "\t=\t0 $$\n",
    " \n",
    "\n",
    "Thus the eigenvalues of $W$\n",
    "  are $ \\frac{1}{\\mu_{1}+\\lambda}\\leq...\\leq\\frac{1}{\\mu_{p}+\\lambda} $\n",
    " ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 1\n",
    "\n",
    "For sufficiently small $ \\lambda>0 $\n",
    " , we have $ MSE(\\hat{\\beta}_{\\lambda}^{R}) < MSE(\\hat{\\beta}) $\n",
    " . \n",
    "\n",
    "#### Proof: \n",
    "\n",
    "\n",
    "$$ \n",
    "MSE(\\hat{\\beta}_{\\lambda}^{R})\n",
    "  = \\mathbb{E}_{\\beta}\\left(\\left\\Vert \\hat{\\beta}_{\\lambda}-\\beta\\right\\Vert ^{2}\\right) $$\n",
    "  $$ =\\mathbb{E}_{\\beta}\\left(\\left\\Vert V\\hat{\\beta}-V\\beta+V\\beta-\\beta\\right\\Vert \\right)\n",
    "  =\t\\mathbb{E}_{\\beta}\\left(\\left\\Vert V\\left(\\hat{\\beta}-\\beta\\right)\\right\\Vert \\right)+\\beta^{T}(V-I)^{T}(V-I)\\beta\n",
    "  $$\n",
    "  $$=\t\\sigma^{2}\\mbox{Tr}\\left(V\\left(X^{T}X\\right)^{-1}V^{T}\\right)+\\lambda^{2}B^{T}W^{T}W\\beta\n",
    "=\t\\sigma^{2}\\mbox{Tr}\\left(\\left(X^{T}X\\right)^{-1}V^{T}V\\right)+\\lambda^{2}\\beta^{T}\\left(PDP^{T}-\\lambda I\\right)^{-1}\\left(PDP^{T}-\\lambda I\\right)^{-1}\\beta\n",
    "\t$$\n",
    "    \n",
    "$$ =\t\\sigma^{2}\\mbox{Tr}\\left(W(I-\\lambda W)\\right)+\\lambda^{2}\\beta^{T}P\\left(D-\\lambda I\\right)^{-2}P^{T}\\beta\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\t\\sigma^{2}\\sum_{j=1}^{p}\\frac{1}{\\mu_{j}+\\lambda}-\\lambda\\sigma^{2}\\sum_{j=1}^{p}\\frac{1}{\\left(\\mu_{j}+\\lambda\\right)^{2}}+\\lambda^{2}\\sum_{j=1}^{p}\\frac{\\alpha_{j}^{2}}{\\left(\\mu_{j}+\\lambda\\right)^{2}}\n",
    "$$\n",
    "\n",
    "$$=\t\\sigma^{2}\\sum_{j=1}^{p}\\frac{\\mu_{j}}{(\\mu_{j}+\\lambda)^{2}}+\\lambda^{2}\\sum_{j=1}^{p}\\frac{\\alpha_{j}^{2}}{\\left(\\mu_{j}+\\lambda\\right)^{2}} \n",
    "$$\n",
    " \n",
    "\n",
    "where $\\alpha=P^{T}\\beta$\n",
    " \n",
    "\n",
    "Thus the variance is a monotone decreasing function of $\\lambda$\n",
    " , while the squared bias is amonotone increasing function of $\\lambda$\n",
    " . We deduce that\n",
    "\n",
    "$$\\frac{d}{d\\lambda}\\mbox{MSE}\\left(\\hat{\\beta}_{\\lambda}^{R}\\right)\t=\t-2\\sigma^{2}\\sum_{j=1}^{p}\\frac{\\mu_{j}}{\\left(\\mu_{j}+\\lambda\\right)^{2}}+\\sum_{j=1}^{p}\\frac{\\left\\{ 2\\left(\\mu_{j}+\\lambda\\right)^{2}\\lambda-2\\lambda^{2}(\\mu_{j}+\\lambda)\\right\\} a_{j}^{2}}{\\left(\\mu_{j}+\\lambda\\right)^{4}}\n",
    "\t=\t-2\\sigma^{2}\\sum_{j=1}^{p}\\frac{\\mu_{j}}{\\left(\\mu_{j}+\\lambda\\right)^{3}}+2\\lambda\\sum_{j=1}^{p}\\frac{\\mu_{j}x_{j}^{2}}{\\left(\\mu_{j}+\\lambda\\right)^{3}}\n",
    "\t<\t0 $$\n",
    " for  $ 0\\leq\\lambda<\\frac{\\sigma^{2}}{\\alpha_{\\max}}$\n",
    " , where  $ \\alpha_{\\max}^{2}=\\max\\left(\\alpha_{1}^{2},...,\\alpha_{p}^{2}\\right)$\n",
    " \n",
    "\n",
    "Unfortunately, a poor choice of $\\lambda$\n",
    "  may substantially increase the MSE. In practice, $\\lambda$\n",
    "  is often chosen by V\n",
    " -fold cross validation. The idea is to randomly split the data into v groups (folds) of roughly the same size. For each $\\lambda$\n",
    "  on a grid of values, and for k=1,..,v\n",
    " , we compute $\\hat{\\beta}_{\\lambda,-k}^{R}$\n",
    " , the ridge regression estimator of $\\beta$\n",
    "  based on all of the data except the k\n",
    " -th fold. Writing $\\kappa(i)$\n",
    "  for the fold to which $\\left(X_{i},\\,Y_{i}\\right)$\n",
    "  belongs, we choose the value of $\\lambda$\n",
    "  that minimises\n",
    "  $CV(\\lambda)=\\sum_{i=1}^{n}\\left(Y_{i}-x_{i}^{T}\\hat{\\beta}_{\\lambda,\\,-k(i)}^{R}\\right)^{2} $\n",
    " \n",
    "\n",
    "This is a fairly reliable way of choosing tuning parameters, although because we use the same data for both the fitting, and assessing the fit, the method can be prone to overfitting (choosing $\\lambda$\n",
    "  too small). Common choices of v\n",
    "  include 5,10\n",
    "  or $n$\n",
    " , the last of these being knwon as 'leave-one-out' cross validation.\n",
    "\n",
    "$V(\\hat{\\beta}-\\beta)\\sim N_{p}\\left(0,\\,\\sigma^{2}V(X^{T}X)^{-1}V^{T}\\right)$\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
